[{"content":" 强化学习的终极目标是求解一个最优策略.\nOverview Not just the map of this course, but also for RL foundations Fundamental tools + Algorithms and methods Importance of different parts Chapter 1: Basic Concpets Concepts: state, action, reward, return, episode, policy Grid-world examples Markov decision process (MDP) Fundamental concepts, widely used later Chapter 2: Bellman Equation One concept. State value $$ v_{\\pi}(s) = \\mathbb{E}[G_t|S_t=s] $$ One tool: Bellman equation $$ v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi}v_{\\pi} $$ Policy evaluation, widely used later Chapter 3: Bellman Optimality Equation A special Bellman equation\nTwo concepts: optimal policy $\\pi^*$ \u0026amp; optimal state value\nOne tool: Bellman optimality equation $$ v = max_{\\pi}(r_{\\pi} + \\gamma P_{\\pi}v) = f(v) $$ Fixed_point theorem Fundamental problems An algorithm solving the equation Optimality, widely used later\nChapter 4: Value Iteration \u0026amp; Policy Iteration First algorithms for optimal policies Three algorithms: Value iteration(VI) Policy iteration(PI) Truncated policy iteration (general case) Policy update and value update, widely used later Need the environment model Chapter 5: Monte Cario Learning Gap: how to do model-free learning? Mean estimation with sampling data $$ \\mathbb{E}[X] \\approx \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i $$ First model-free RL algorthims Algorithms: MC Basic MC Exploring Starts MC $\\epsilon$-greedy Chapter 6: Stochastic Approximation Gap: from non-incremental to incremental Mean estimation Algorithms: Robbins-Monro(RM) algorithm Stochastic gradient descent(SGD) SGD, BGD, MBGD Incremental manner and BGD, widely used later Chapter 7: Temporal-Difference Learning Classic RL algorithms Algorithms: TD learning of state values Sarsa: TD learning of action values Q-learning: TD learning of optimal action values (off-policy) Unified point of view Chapter 8: Value Function Approximation Gap: tabular representation to function representation Algorithms: State value estimation with value function approximation(VFA): $$ min_{w} J(w) = \\mathbb{E}[v_{\\pi}(S) - \\hat{v}(S,w)] $$ Sarsa with VFA Q-learning with VFA Deep Q-learning Neural networks come into RL Chapter 9: Policy Gradient Methods Gap: from value-based to policy-based Contents: Metrics to define optimal policies: $$ J(\\theta) = \\bar{v}_{\\pi},\\bar{r}_{\\pi} $$ Policy gradient: $$ \\Delta J(\\theta) = \\mathbb{E}[\\Delta_{\\theta} ln {\\pi}(A|S,\\theta)q_{\\pi}(S,A)] $$ Gradient-ascent algorithm(REINFORCE) $$ \\theta_{t+1} = \\theta_{t} + \\alpha \\Delta_{\\theta} ln \\pi(a_{t}|s_t, \\theta_t)q_t(s_t,a_t) $$ Chapter 10: Actor-Critic Methods Gap: policy-based + value-baesd $$ \\theta_{t+1} = \\theta_{t} + \\alpha \\Delta_{\\theta} ln \\pi(a_{t}|s_t, \\theta_t)q_t(s_t,a_t) $$ Algorithms: The simplest actor-critc(QAC) Advantage actor-critic(A2C) OFF-policy actor-critic Importance sampling Deterministic actor-critic(DPG) ","date":"2025-12-18T00:00:00Z","image":"https://nopanicer.github.io/p/ai-dev/image_hu_ed7ceba178c433ec.png","permalink":"https://nopanicer.github.io/p/ai-dev/","title":"强化学习"},{"content":"机器学习统计基础 ","date":"2025-12-01T00:00:00Z","image":"https://nopanicer.github.io/images/statistic.png","permalink":"https://nopanicer.github.io/p/statistic-blog/","title":"hugo上手指南"},{"content":"智能体(intelligent agent)的概念是贯穿整本书的主题思想。我们将人工智能定义为对从 环境中接收感知并执行动作的智能体的研究。每个这样的智能体都要实现一个将感知序列映射 为动作的函数,我们介绍了表示这些函数的不同方法,如反应型智能体、实时规划器、决策论 系统和深度学习系统。我们强调,学习既是构造良好系统的方法,也是将设计者的影响范围扩 展到未知环境的方法。我们没有把机器人学和视觉看作独立定义的问题,而是将其看作实现目 标的服务。我们强调任务环境在确定合适的智能体设计中的重要性\n人工智能，字面意思就是人类生成的智能，也就是人类对智能对脑中智能概念的模仿，所以要回答什么是人工智能，就必须回答什么是智能。依据智能定义的不同，人工智能的实现形式也会稍加区别。\n正确的思考/正确的行为，理性/感性 有些根据对人类行为的复刻来定义智能,而另一些更喜欢用“理性” (rationality)来抽象正式地定义智能,直观上的理解是做“正确的事情”。智能主题的本身也各 不相同:一些人将智能视为内部思维过程和推理的属性,而另一些人则关注智能的外部特征, 也就是智能行为。\n什么是智能-哲学 如何定义智能-逻辑学 如何产生智能-脑科学 如何运转智能-经济学 如何量化智能-统计学 如何模拟智能-计算机科学 ","date":"2025-11-02T00:00:00Z","image":"https://nopanicer.github.io/p/ai-dev/image_hu_ed7ceba178c433ec.png","permalink":"https://nopanicer.github.io/p/ai-dev/","title":"人工智能之旅"},{"content":"","date":"2025-10-19T00:00:00Z","permalink":"https://nopanicer.github.io/p/git/","title":"Git使用学习"},{"content":"Why we need World Models 知识训练——\u0026gt;特征提取——\u0026gt;数据驱动\n人工智能，21世纪时下最热门的议题，走过了他的第 69 个春秋。在1956 年的达特茅斯会议上，涌现了一批以Minskey为首的人工智能学家。所谓人工智能就是对智能的模仿，是一种数字化，程序化智能的方式。秉持着这种思想，最早的人工智能学者们认为智能来源于逻辑能力，毕竟人类历史上最伟大的思想都是来源于逻辑的推断。 Minskey 等人通过构筑 GTP 等成果实现了自动推理的机器，这种机器通过将知识与规则编码入程序之中，可以依照某种程序化的范本完成一些智能的行为，实现简单命题的证明，这种智能哲学后来被称为符号主义。当时人们对这种自动定理证明机满怀信心，认为只要给定足够多的规则，足够完善的信息，就可以实现同人类相当的智能。专家系统如雨后春笋般出现，并快速得在人类历史中产生影响。但随着时间的推理，人们发现这种固定化的程序并不适用现实生活————原因在于现实中的环境时常发生变动，所以需要有人持续维护程序中的规则和知识。这种体力劳动并不比完全不使用专家系统轻松，所以没多长时间，这种专家系统几乎销声匿迹了。符号主义对于实现形式化验证是一种极大的助力，但是没有证据表明这种机器能够涌现出自我推理的创造力。 这之后的人工智能学家仅仅拥有逻辑的机器是称不上智能的，类似于人类会对现实中的事物观察，归纳，总结规律，智能也应当拥有总结和抽象概括的能力，换言之，应当具有模式识别的能力。万幸的是，基于归纳数学家已经发展出了一套完善的方法，将统计学习的思想带入人工智能，我们可以将智能理解为建立在某种模型之上对于参数的学习。比如给出一个地区的历年的方法，我们可以选取不同的多项式模型拟合，使用最小二乘法，我们就可以学习到在给定模型下最优的参数，变相的，我们相当于学习到了数据背后的“规律”。关于具体的模型选取这部分非常复杂，正确的模型能够极好的预测未学习的数据，而错误的模型只能在拟合效果上优良，在真实环境中表现一塌糊涂。特征工程随之成为了当时机器学习的主要目标，人们争先根据自己的经验、对智能的理解建立大量的特征，以此训练出许多模型。这些模型从现在的角度来看效果也是不错的，但是这种学习属于特化学习，即便目标看起来相似的两种模型，其需要的特征也未必相同（如同样是图片识别任务，识别景物和识别小动物的特征选取或许就完全不同），且受限于现实世界中任务的复杂多样，以这种方式训练出来的模型泛化效果很差，缺乏实现多种任务的能力。 基于特征提取的统计机器学习最大的不便之处在于手动设计特征效果低下。通过一种简单的思想我们可以省去这种麻烦————复杂来源于简单的叠加，即便是使用简单的线性函数（加上一点不那么线性的函数），也可以模拟复杂的特征所表示的信息。我们可以让机器模拟神经元来产生于类似脑信号的效果，神经细胞突触末端可以传递信号，一些信号表现为激活特性，而另一些则表示为抑制特性，当同一细胞接收到的信号激活大于抑制时，我们可以继续激活下一层神经细胞，反之信号则终止传递（事实上，我这里所说也正是所有深度网络中发生的事情）。人工智能的研究步入连接主义的时代，模型的训练步入数据驱动的阶段，我们只需要通过大量数据灌输训练，模型可以自己发现数据背后的规律，这种智能在某种程度上达到甚至超过了人类水平。通过使用人类信息时代产生的几乎所有数据，基于 transformer 的大语言模型已经在自然语言处理领域达到了极强的水平。从知识水平的高低来看，GPT 类模型已经足够强大，但是我们必须发问，智能是否仅仅等于知识？\n精炼版：为何当下必须讨论 World Models 回顾上面保留的原始发展脉络，可以抽象为一条“表示方式演进”与“泛化方式演进”的双线：\n符号主义（规则→推理）：显式知识 + 演绎，缺乏适应性与自动扩展。 统计学习（模型→参数）：用归纳取代纯规则，但强依赖手工特征与任务定制。 特征工程阶段（经验→特化）：专家主导表示，迁移差、成本高。 连接主义 / 深度学习（分层→自动抽象）：端到端表示学习替代手工特征，但对外部世界的结构理解仍薄弱。 大模型时代（海量数据→模式压缩）：语言与多模态模式被高度建模，但缺乏稳定的“世界因果 + 可操作物理 + 长期规划”能力。 因此出现瓶颈：仅依靠“被动模式匹配”与“记忆式知识复用”不足以支撑真正的通用智能（AGI）与跨任务迁移。我们需要能在以下维度前移的机制：\n状态建模：从“文本/像素表面”到“潜在可操作世界状态（Latent actionable state）”。 因果与可证伪：能够回答“如果我改变 X，Y 会怎样”，并支持反事实比较。 长期预测：不只是下一 token / 下一帧，而是跨时间跨度稳定滚动模拟。 主动探索：在未知下建立不确定性表征与信息增益策略（curiosity / intrinsic motivation）。 规划与控制接口：让模型输出可以映射为环境可执行动作序列而非仅自然语言。 错误内省与自我修正：具备“模型→环境→偏差诊断→局部更新”闭环。 资源效率：少样本、离线数据再利用、世界模型内部生成合成经验（dreaming / imagination）。 World Models 正是对上述能力的承载：它不是简单的“更大的语言模型”，而是一套把感知、记忆、预测、规划、执行、纠错统合为一个可迭代循环的结构化系统。其核心思想：\n使用分层表征（例如：像素→对象→关系→动态方程）。 将环境动态显式参数化（或隐式潜变量学习），支持 rollout。 通过内部模拟减少真实交互成本（model-based RL / imagination-based planning）。 在策略生成前先进行多分支前瞻（branching futures），筛选风险与收益。 将错误（预测残差）直接反馈到世界模型更新，而非仅更新策略头。 支持跨领域迁移：世界结构部分复用，不同任务只更换目标函数。 与“仅知识累计”相比，World Models 关注“结构 + 动态 + 可操作性”。它的出现使得智能从“被动回答”转向“主动构思与试验”，为：\n更安全的决策（提前模拟后果）。 更强的鲁棒性（对未知输入有预测与回退机制）。 更好的可解释性（内部状态可以与真实世界因素映射）。 更高的样本效率（生成内部经验，减少外部试错）。 一句话总结：World Models 让 AI 不再只是“记住世界”，而是“内部重建并可操纵一个世界”，从而迈向具备理解、预测、规划与自我修复能力的下一代智能系统。\n注：以上为结构化浓缩，保留原始段落以供风格与语义对照。\n世界模型是什么 如果AGI实现了，那不就是可以通过AGI生成游戏世界了 世界模型需要什么， 面对未知得能力 JEPA原来是这么个东西 Understanding Predicting Planning 世界需要理解错误，能纠错，需要自己解决bug 世界模型是人工智能系统对外部世界的内部表征或模拟。它就像AI的“大脑”中的一个虚拟沙盒，让AI能够理解、预测和推理现实世界的运作方式，包括物理规律、因果关系和物体运动。\n我们人类在做任何事情之前，都会在脑海中对即将发生的事情进行模拟和预测。比如，一个棒球运动员能够击中告诉飞来的棒球，是因为他的大脑能够快速预测棒球的飞行轨迹，并在潜意识中做出反应。这种预测能力，就是基于他对世界的内部模型。\n一个世界模型通常包含以下核心能力：\n内部表征（Internal Representation） : 将高维的原始数据编码为低维、简洁的潜在状态，形成对世界的抽象和概括。 未来预测（Future Prediction）： 基于当前状态和采取的行动，预测未来可能发生的状态变化。 因果推理（Causal Reasoning）： 理解“如果……会发生什么”的问题，即能够进行反事实推理。1 参考文献 https://www.bilibili.com/video/BV1GXfCYYEw6/?spm_id_from=333.337.search-card.all.click\u0026vd_source=86c342ca5a44bf015c4bf3934ae1b354\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-10-19T00:00:00Z","permalink":"https://nopanicer.github.io/p/world-models/","title":"World Models"},{"content":"正文测试 引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 ","date":"2025-10-17T00:00:00Z","image":"https://nopanicer.github.io/p/hugo-blog/image_hu_a82eee42a02adcb3.png","permalink":"https://nopanicer.github.io/p/hugo-blog/","title":"hugo上手指南"}]